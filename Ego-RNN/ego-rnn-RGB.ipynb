{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "389pw3Ang7sD"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from objectAttentionModelConvLSTM import *\n",
    "from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n",
    "                                RandomHorizontalFlip)\n",
    "from makeDatasetFrame import *\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "VAL_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DnxYxClIg0eE"
   },
   "outputs": [],
   "source": [
    "def main_run(version, stage, train_data_dir, stage1_dict, out_dir, seqLen, trainBatchSize,\n",
    "             valBatchSize, numEpochs, lr1, decay_factor, decay_step, mem_size):\n",
    "    num_classes = 61\n",
    "\n",
    "    model_folder = os.path.join(\"./\", out_dir, version)\n",
    "\n",
    "    if os.path.exists(model_folder):\n",
    "        print('Directory {} exists!'.format(model_folder))\n",
    "        sys.exit()\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "    train_log_loss = open((model_folder + '/train_log_loss.txt'), 'w')\n",
    "    train_log_acc = open((model_folder + '/train_log_acc.txt'), 'w')\n",
    "    val_log_loss = open((model_folder + '/val_log_loss.txt'), 'w')\n",
    "    val_log_acc = open((model_folder + '/val_log_acc.txt'), 'w')\n",
    "\n",
    "    # Train val partitioning\n",
    "    train_usr = [\"S1\", \"S3\", \"S4\"]\n",
    "    val_usr = [\"S2\"]\n",
    "\n",
    "    # Data loader\n",
    "    normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    spatial_transform = Compose(\n",
    "        [Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
    "         ToTensor(), normalize])\n",
    "\n",
    "    vid_seq_train = makeDataset(train_data_dir, train_usr,\n",
    "                                spatial_transform=spatial_transform, seqLen=seqLen)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(vid_seq_train, batch_size=trainBatchSize,\n",
    "                                               shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    vid_seq_val = makeDataset(train_data_dir, val_usr,\n",
    "                              spatial_transform=Compose([Scale(256), CenterCrop(224), ToTensor(), normalize]),\n",
    "                              seqLen=seqLen)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(vid_seq_val, batch_size=valBatchSize,\n",
    "                                             shuffle=False, num_workers=2, pin_memory=True, phase=\"test\")\n",
    "\n",
    "    train_params = []\n",
    "\n",
    "    # stage 1: train only lstm\n",
    "    if stage == 1:\n",
    "\n",
    "        model = attentionModel(num_classes=num_classes, mem_size=mem_size)\n",
    "        model.train(False)\n",
    "        for params in model.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    # stage 2: train lstm, layer4, spatial attention and final fc\n",
    "    else:\n",
    "        model = attentionModel(num_classes=num_classes, mem_size=mem_size)\n",
    "        model.load_state_dict(torch.load(stage1_dict))  # pretrained\n",
    "        model.train(False)\n",
    "        for params in model.parameters():\n",
    "            params.requires_grad = False\n",
    "        #\n",
    "        for params in model.resNet.layer4[0].conv1.parameters():\n",
    "            params.requires_grad = True\n",
    "            train_params += [params]\n",
    "\n",
    "        for params in model.resNet.layer4[0].conv2.parameters():\n",
    "            params.requires_grad = True\n",
    "            train_params += [params]\n",
    "\n",
    "        for params in model.resNet.layer4[1].conv1.parameters():\n",
    "            params.requires_grad = True\n",
    "            train_params += [params]\n",
    "\n",
    "        for params in model.resNet.layer4[1].conv2.parameters():\n",
    "            params.requires_grad = True\n",
    "            train_params += [params]\n",
    "\n",
    "        for params in model.resNet.layer4[2].conv1.parameters():\n",
    "            params.requires_grad = True\n",
    "            train_params += [params]\n",
    "        #\n",
    "        for params in model.resNet.layer4[2].conv2.parameters():\n",
    "            params.requires_grad = True\n",
    "            train_params += [params]\n",
    "        for params in model.resNet.fc.parameters():  # fully connected layer\n",
    "            params.requires_grad = True\n",
    "            train_params += [params]\n",
    "\n",
    "        model.resNet.layer4[0].conv1.train(True)\n",
    "        model.resNet.layer4[0].conv2.train(True)\n",
    "        model.resNet.layer4[1].conv1.train(True)\n",
    "        model.resNet.layer4[1].conv2.train(True)\n",
    "        model.resNet.layer4[2].conv1.train(True)\n",
    "        model.resNet.layer4[2].conv2.train(True)\n",
    "        model.resNet.fc.train(True)\n",
    "\n",
    "    for params in model.lstm_cell.parameters():  # for both stages we train the lstm\n",
    "        params.requires_grad = True\n",
    "        train_params += [params]\n",
    "\n",
    "    for params in model.classifier.parameters():  # for both stages we train the last classifier (after the lstm and avg pooling)\n",
    "        params.requires_grad = True\n",
    "        train_params += [params]\n",
    "\n",
    "    model.lstm_cell.train(True)\n",
    "\n",
    "    model.classifier.train(True)\n",
    "    model.cuda()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer_fn = torch.optim.Adam(train_params, lr=lr1, weight_decay=4e-5, eps=1e-4)\n",
    "\n",
    "    optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=decay_step,\n",
    "                                                           gamma=decay_factor)\n",
    "\n",
    "    train_iter = 0\n",
    "    min_accuracy = 0\n",
    "\n",
    "    for epoch in range(numEpochs):\n",
    "        optim_scheduler.step()\n",
    "        epoch_loss = 0\n",
    "        numCorrTrain = 0\n",
    "        trainSamples = 0\n",
    "        iterPerEpoch = 0\n",
    "        model.lstm_cell.train(True)\n",
    "        model.classifier.train(True)\n",
    "        if stage == 2:\n",
    "            model.resNet.layer4[0].conv1.train(True)\n",
    "            model.resNet.layer4[0].conv2.train(True)\n",
    "            model.resNet.layer4[1].conv1.train(True)\n",
    "            model.resNet.layer4[1].conv2.train(True)\n",
    "            model.resNet.layer4[2].conv1.train(True)\n",
    "            model.resNet.layer4[2].conv2.train(True)\n",
    "            model.resNet.fc.train(True)\n",
    "        for i, (inputs, inputsF, targets) in enumerate(train_loader):\n",
    "            train_iter += 1\n",
    "            iterPerEpoch += 1\n",
    "            optimizer_fn.zero_grad()\n",
    "            inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).to(DEVICE))\n",
    "            labelVariable = Variable(targets.to(DEVICE))\n",
    "            trainSamples += inputs.size(0)\n",
    "            output_label, _ = model(inputVariable)\n",
    "            loss = loss_fn(output_label, labelVariable)\n",
    "            loss.backward()\n",
    "            optimizer_fn.step()\n",
    "            _, predicted = torch.max(output_label.data, 1)\n",
    "            numCorrTrain += (predicted == targets.to(DEVICE)).sum()  # evaluating number of correct classifications\n",
    "            epoch_loss += loss.data.item()\n",
    "        avg_loss = epoch_loss / iterPerEpoch\n",
    "        trainAccuracy = (numCorrTrain.data.item() / trainSamples)\n",
    "\n",
    "        train_log_loss.write('Training loss after {} epoch = {}\\n'.format(epoch + 1, avg_loss))  # log file\n",
    "        train_log_acc.write('Training accuracy after {} epoch = {}\\n'.format(epoch + 1, trainAccuracy))  # log file\n",
    "        print('Train: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch + 1, avg_loss, trainAccuracy))\n",
    "\n",
    "        if (epoch + 1) % VAL_FREQUENCY == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            val_iter = 0\n",
    "            val_samples = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, inputsF, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                val_samples += inputs.size(0)\n",
    "                inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).to(DEVICE))\n",
    "                labelVariable = Variable(targets.to(DEVICE))\n",
    "                output_label, _ = model(inputVariable)\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                val_loss_epoch += val_loss.data.item()\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += (predicted == targets.to(DEVICE)).sum()  # evaluating number of correct classifications\n",
    "            val_accuracy = (numCorr.data.item() / val_samples)\n",
    "            avg_val_loss = val_loss_epoch / val_iter\n",
    "            print('Val: Epoch = {} | Loss {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            val_log_loss.write('Val Loss after {} epochs = {}\\n'.format(epoch + 1, avg_val_loss))  # log file\n",
    "            val_log_acc.write('Val Accuracy after {} epochs = {}%\\n'.format(epoch + 1, val_accuracy))  # log file\n",
    "            if val_accuracy > min_accuracy:\n",
    "                save_path_model = (\n",
    "                        model_folder + '/model_rgb_state_dict.pth')  # every epoch, check if the val accuracy is improved, if so, save that model\n",
    "                torch.save(model.state_dict(),\n",
    "                           save_path_model)  # in that way, even if the model overfit, you will get always the best model\n",
    "                min_accuracy = val_accuracy  # in this way you don't have to care too much about the number of epochs\n",
    "\n",
    "    train_log_loss.close()\n",
    "    train_log_acc.close()\n",
    "    val_log_acc.close()\n",
    "    val_log_loss.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __main__():\n",
    "    version = \"rgb_16frames\"\n",
    "    trainDatasetDir = \"/content/\"\n",
    "    outDir = \"/results/\"\n",
    "    stage1Dict = \"./\" + outDir + \"/\" + version + \"_1/model_rgb_state_dict.pth\"  # args.stage1Dict\n",
    "\n",
    "    # STAGE 1 PARAMETERS\n",
    "    ST1_seqLen = 16  # 7\n",
    "    ST1_trainBatchSize = 32  # 32\n",
    "    ST1_valBatchSize = 32  # 32\n",
    "    ST1_numEpochs = 200  # 200\n",
    "    ST1_lr1 = 1e-3  # 1e-3\n",
    "    ST1_stepSize = [25, 75, 150]  # [25, 75, 150]\n",
    "    ST1_decayRate = 0.1  # 0.1\n",
    "    ST1_memSize = 512  # 512\n",
    "\n",
    "    # STAGE 2 PARAMETERS\n",
    "    ST2_seqLen = 16  # 7\n",
    "    ST2_trainBatchSize = 32  # 32\n",
    "    ST2_valBatchSize = 32  # 32\n",
    "    ST2_numEpochs = 150  # 150\n",
    "    ST2_lr1 = 1e-4  # 1e-4\n",
    "    ST2_stepSize = [25, 75]  # [25, 75]\n",
    "    ST2_decayRate = 0.1  # 0.1\n",
    "    ST2_memSize = 512  # 512\n",
    "\n",
    "    # STAGE 1\n",
    "    \n",
    "    main_run(version + \"_1\",\n",
    "             stage=1,\n",
    "             train_data_dir=trainDatasetDir,\n",
    "             stage1_dict=stage1Dict,\n",
    "             out_dir=outDir,\n",
    "             seqLen=ST1_seqLen,\n",
    "             trainBatchSize=ST1_trainBatchSize,\n",
    "             valBatchSize=ST1_valBatchSize,\n",
    "             numEpochs=ST1_numEpochs,\n",
    "             lr1=ST1_lr1,\n",
    "             decay_factor=ST1_decayRate,\n",
    "             decay_step=ST1_stepSize,\n",
    "             mem_size=ST1_memSize)\n",
    "    \n",
    "    # STAGE 2\n",
    "    main_run(version + \"_2\",\n",
    "             stage=2,\n",
    "             train_data_dir=trainDatasetDir,\n",
    "             stage1_dict=stage1Dict,\n",
    "             out_dir=outDir,\n",
    "             seqLen=ST2_seqLen,\n",
    "             trainBatchSize=ST2_trainBatchSize,\n",
    "             valBatchSize=ST2_valBatchSize,\n",
    "             numEpochs=ST2_numEpochs,\n",
    "             lr1=ST2_lr1,\n",
    "             decay_factor=ST2_decayRate,\n",
    "             decay_step=ST2_stepSize,\n",
    "             mem_size=ST2_memSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNYyOE7Mqm3FPDTHHYTBBPc",
   "collapsed_sections": [],
   "name": "ego-rnn-RGB_LDN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
