{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"getAttentionMap.ipynb","provenance":[],"mount_file_id":"1YtFVeNOQkROHTsR25JG2tHBnLpSK5B8r","authorship_tag":"ABX9TyOB8Krjp/nvTWYENvWXHWIf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6N_lrpKA1UoN","colab_type":"code","outputId":"efdd977d-ae05-4fbd-ce89-4526929432d2","executionInfo":{"status":"ok","timestamp":1591739780440,"user_tz":-120,"elapsed":2707,"user":{"displayName":"Laboratorio MLDL","photoUrl":"","userId":"15250599834567100244"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd \"/content/drive/My Drive/Lorenzo/ego-rnn-ss-task\""],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Lorenzo/ego-rnn-ss-task\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rokkn0LL0NYp","colab_type":"code","colab":{}},"source":["import numpy as np\n","from torchvision import transforms\n","import cv2\n","from objectAttentionModelConvLSTM import *\n","from attentionMapModel import attentionMap\n","from PIL import Image\n","import os\n","import imageio"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"53e23GdF0RRJ","colab_type":"code","outputId":"e18a2cc2-1aa0-4051-e9f6-cc80804b6ec6","executionInfo":{"status":"ok","timestamp":1591739938068,"user_tz":-120,"elapsed":10298,"user":{"displayName":"Laboratorio MLDL","photoUrl":"","userId":"15250599834567100244"}},"colab":{"base_uri":"https://localhost:8080/","height":434}},"source":["DEVICE = \"cuda\"\n","\n","####################Model definition###############################\n","num_classes = 61  # Classes in the pre-trained model\n","mem_size = 512\n","#model_state_dict = '/content/drive/My Drive/FINAL_LOGS/200+150epochs_RGB_16frames/test_2/model_rgb_state_dict.pth'  # Weights of the pre-trained model\n","#model_state_dict = \"/content/drive/My Drive/Lorenzo/ego-rnn-latest/200+150epochs_RGB_16frames/test_2/model_rgb_state_dict.pth\"\n","model_state_dict = \"/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/rgb_16frames_regression_kl/model_rgb_state_dict.pth\"\n","model = attentionModel(num_classes=num_classes, mem_size=mem_size)\n","model.load_state_dict(torch.load(model_state_dict), strict=False)\n","model_backbone = model.resNet\n","attentionMapModel = attentionMap(model_backbone).to(DEVICE)\n","attentionMapModel.train(False)\n","for params in attentionMapModel.parameters():\n","    params.requires_grad = False\n","###################################################################"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:31: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_i_xx.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:32: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  torch.nn.init.constant(self.conv_i_xx.bias, 0)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:33: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_i_hh.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:35: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_f_xx.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:36: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  torch.nn.init.constant(self.conv_f_xx.bias, 0)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:37: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_f_hh.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:39: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_c_xx.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  torch.nn.init.constant(self.conv_c_xx.bias, 0)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:41: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_c_hh.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:43: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_o_xx.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:44: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  torch.nn.init.constant(self.conv_o_xx.bias, 0)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:45: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_o_hh.weight)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"VYWn0PEz0WQY","colab_type":"code","outputId":"d788e59f-fe28-44bb-9a0f-1d9b89c789a3","executionInfo":{"status":"ok","timestamp":1591739938073,"user_tz":-120,"elapsed":10296,"user":{"displayName":"Laboratorio MLDL","photoUrl":"","userId":"15250599834567100244"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["normalize = transforms.Normalize(\n","    mean=[0.485, 0.456, 0.406],\n","    std=[0.229, 0.224, 0.225]\n",")\n","\n","preprocess1 = transforms.Compose([\n","    transforms.Scale(256),\n","    transforms.CenterCrop(224),\n","])\n","\n","preprocess2 = transforms.Compose([\n","    transforms.ToTensor(),\n","    normalize])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:211: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n","  \"please use transforms.Resize instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_pi_IyiV0HCt","colab_type":"code","outputId":"01882301-9b13-4503-d341-a2492aecf45d","executionInfo":{"status":"ok","timestamp":1591739949849,"user_tz":-120,"elapsed":3634,"user":{"displayName":"Laboratorio MLDL","photoUrl":"","userId":"15250599834567100244"}},"colab":{"base_uri":"https://localhost:8080/","height":399}},"source":["fl_name_in = './attention_test/frames/'\n","fl_name_out = './attention_test/open_ketchup/'\n","\n","# load video and get frames #\n","\n","'''\n","cam = cv2.VideoCapture(\"./attention_test/grab_glass.mp4\")            # load video \n","\n","currentframe = 0\n","while (True):\n","    ret, frame = cam.read()\n","    if ret:\n","        name = fl_name_in + \"/\" + str(currentframe).zfill(3) + '.jpg'\n","        cv2.imwrite(name, frame)\n","        currentframe += 1\n","    else:\n","        break\n","'''\n","\n","frames = []\n","for i, img_name in enumerate(sorted(os.listdir(fl_name_in))):\n","    img_pil = Image.open(os.path.join(fl_name_in, img_name))                    # open image\n","    img_pil1 = preprocess1(img_pil)                                             # apply preprocessing (scaling and cropping)\n","    img_size = img_pil1.size\n","    size_upsample = (img_size[0], img_size[1])\n","    img_tensor = preprocess2(img_pil1)                                          # convert to tensor\n","    img_variable = Variable(img_tensor.unsqueeze(0).to(DEVICE))                 # send to gpu\n","    img = np.asarray(img_pil1)                                                  # keep original image (transformed)\n","    attentionMap_image = attentionMapModel(img_variable, img, size_upsample)    # compute attention map and stack it on original image\n","    cv2.imwrite(os.path.join(fl_name_out, img_name), attentionMap_image)        # save image\n","\n","    frames.append(imageio.imread(os.path.join(fl_name_out, img_name)))          # keep image, read with imageio\n","\n","    print(f\"frame_{i}\")\n","\n","kargs = {'duration': 0.05}                                                      # duration of single frame\n","imageio.mimsave(fl_name_out + \"/frames.gif\", frames, **kargs)                   # create gif"],"execution_count":8,"outputs":[{"output_type":"stream","text":["frame_0\n","frame_1\n","frame_2\n","frame_3\n","frame_4\n","frame_5\n","frame_6\n","frame_7\n","frame_8\n","frame_9\n","frame_10\n","frame_11\n","frame_12\n","frame_13\n","frame_14\n","frame_15\n","frame_16\n","frame_17\n","frame_18\n","frame_19\n","frame_20\n","frame_21\n"],"name":"stdout"}]}]}