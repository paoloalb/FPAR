{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ego-rnn-RGB_LDN.ipynb","provenance":[{"file_id":"1-1zSpoxck79TSjQnVol85KGqeEIEErQR","timestamp":1590511191352}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2OU3pQgxiN8n","colab_type":"code","outputId":"3076ac69-70ba-43d4-b701-d0659bc43f48","executionInfo":{"status":"ok","timestamp":1591543643954,"user_tz":-120,"elapsed":921,"user":{"displayName":"Laboratorio MLDL","photoUrl":"","userId":"15250599834567100244"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eUJ6IA36Y9Pp","colab_type":"code","outputId":"31c53ccb-c721-4700-f456-9b153296529c","executionInfo":{"status":"ok","timestamp":1591543662520,"user_tz":-120,"elapsed":19451,"user":{"displayName":"Laboratorio MLDL","photoUrl":"","userId":"15250599834567100244"}},"colab":{"base_uri":"https://localhost:8080/","height":411}},"source":["%cd \"/content/\"\n","!wget -O \"frames.tar.xz\" \"https://www.dropbox.com/s/etj53uan64h5csc/processed_frames.tar.xz?dl=0\""],"execution_count":11,"outputs":[{"output_type":"stream","text":["/content\n","--2020-06-07 15:27:24--  https://www.dropbox.com/s/etj53uan64h5csc/processed_frames.tar.xz?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.67.1, 2620:100:6023:1::a27d:4301\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.67.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/etj53uan64h5csc/processed_frames.tar.xz [following]\n","--2020-06-07 15:27:25--  https://www.dropbox.com/s/raw/etj53uan64h5csc/processed_frames.tar.xz\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uca9382b82fcd2259286b0b60c78.dl.dropboxusercontent.com/cd/0/inline/A5O-D_qtOG3JE28vFmfRY3f0GwXEzn3x_pZEFU2gm01jaNBqzCZxjZbFbVwswcH8M2KSZUwBQsc2UNSID770s6BtpkS9UDtmih2SZZ3nh7-X7qLQyRJ9hCf_ddchWO7KEA0/file# [following]\n","--2020-06-07 15:27:25--  https://uca9382b82fcd2259286b0b60c78.dl.dropboxusercontent.com/cd/0/inline/A5O-D_qtOG3JE28vFmfRY3f0GwXEzn3x_pZEFU2gm01jaNBqzCZxjZbFbVwswcH8M2KSZUwBQsc2UNSID770s6BtpkS9UDtmih2SZZ3nh7-X7qLQyRJ9hCf_ddchWO7KEA0/file\n","Resolving uca9382b82fcd2259286b0b60c78.dl.dropboxusercontent.com (uca9382b82fcd2259286b0b60c78.dl.dropboxusercontent.com)... 162.125.67.15, 2620:100:6023:15::a27d:430f\n","Connecting to uca9382b82fcd2259286b0b60c78.dl.dropboxusercontent.com (uca9382b82fcd2259286b0b60c78.dl.dropboxusercontent.com)|162.125.67.15|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /cd/0/inline2/A5MubiGhIIt8Z5-F8Ri82fK950YjGJbd51wonkyt-Xr2moZDKSmVaq3DOSZpIGdA2KdKAxwmVvOf-b8iq79igPRYekMGCdIG0jrMKfbDUhF0aorr5FWgTvM4c52tptFcFabat0L2q9vVCGNGmTmjCq7lCdSO9ye2lFX3FlXemp2gtgKMjRWXJvS1uSNA45TYVzcad_AtH2fN_5ld0xsE_G6vqc6Wd5VJEEDRV8qMn7Hau_C1vaArEMrCToEbH2ZJJDMnQOpY5FYpBNkBU6q-cqJ79inMmYitdt3fv4XMLpISinia5sm_QlXtVb22OMj_ltWDzcNDOKoEsvp9ovGAwf0uIvN8HnGAZjR3Nh37FZ3_9g/file [following]\n","--2020-06-07 15:27:26--  https://uca9382b82fcd2259286b0b60c78.dl.dropboxusercontent.com/cd/0/inline2/A5MubiGhIIt8Z5-F8Ri82fK950YjGJbd51wonkyt-Xr2moZDKSmVaq3DOSZpIGdA2KdKAxwmVvOf-b8iq79igPRYekMGCdIG0jrMKfbDUhF0aorr5FWgTvM4c52tptFcFabat0L2q9vVCGNGmTmjCq7lCdSO9ye2lFX3FlXemp2gtgKMjRWXJvS1uSNA45TYVzcad_AtH2fN_5ld0xsE_G6vqc6Wd5VJEEDRV8qMn7Hau_C1vaArEMrCToEbH2ZJJDMnQOpY5FYpBNkBU6q-cqJ79inMmYitdt3fv4XMLpISinia5sm_QlXtVb22OMj_ltWDzcNDOKoEsvp9ovGAwf0uIvN8HnGAZjR3Nh37FZ3_9g/file\n","Reusing existing connection to uca9382b82fcd2259286b0b60c78.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3579197744 (3.3G) [application/octet-stream]\n","Saving to: ‘frames.tar.xz’\n","\n","frames.tar.xz        10%[=>                  ] 355.39M  24.4MB/s    eta 2m 5s  ^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jozu0Z_Rsbt7","colab_type":"code","colab":{}},"source":["!tar \"xf\" \"frames.tar.xz\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZGm1Yqo5NC0","colab_type":"code","colab":{}},"source":["def mmaps_preparation(data, threshold):\n","  maps = []\n","  for i in range(data.shape[0]):\n","    tensor = data[i]\n","    tensor = F.interpolate(tensor, size=(7,7), mode='bilinear')\n","    #tensor[tensor > threshold] = 1\n","    #tensor[tensor <= threshold] = 0\n","    maps.append(tensor)\n","  \n","  maps = torch.stack(maps, 0).squeeze(2)\n","  bz, nc, h, w = maps.size()\n","  maps = maps.view(bz, nc, h*w)\n","\n","  return maps  \n","\n","\n","def loss_ms_fn(ms_out, map_out):\n","\n","    #final_loss = F.binary_cross_entropy(ms_out, map_out.to(DEVICE))\n","    #final_loss = F.soft_margin_loss(ms_out, map_out.to(DEVICE))\n","    #final_loss = F.mse_loss(ms_out, map_out.to(DEVICE))\n","    final_loss = F.kl_div(ms_out, map_out.to(DEVICE))\n","\n","    print(\"MS loss: %f\" % final_loss)\n","    return final_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oBoVC4ZQia5z","colab_type":"code","outputId":"b17e51b4-33ee-43a5-dcca-e7b029313d83","executionInfo":{"status":"ok","timestamp":1591543671678,"user_tz":-120,"elapsed":1387,"user":{"displayName":"Laboratorio MLDL","photoUrl":"","userId":"15250599834567100244"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd \"/content/drive/My Drive/Lorenzo/ego-rnn-ss-task\""],"execution_count":13,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Lorenzo/ego-rnn-ss-task\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"389pw3Ang7sD","colab_type":"code","colab":{}},"source":["from __future__ import print_function, division\n","from objectAttentionModelConvLSTM import *\n","from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n","                                RandomHorizontalFlip)\n","from makeDatasetFrame import *\n","import argparse\n","import sys\n","import time\n","\n","DEVICE = \"cuda\"\n","VAL_FREQUENCY = 5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DnxYxClIg0eE","colab_type":"code","colab":{}},"source":["def main_run(version, stage, train_data_dir, stage1_dict, out_dir, seqLen, trainBatchSize,\n","             valBatchSize, numEpochs, lr1, decay_factor, decay_step, mem_size):\n","    num_classes = 61\n","\n","    model_folder = os.path.join(\"./\", out_dir, version)\n","\n","    if os.path.exists(model_folder):\n","        print('Directory {} exists!'.format(model_folder))\n","        sys.exit()\n","    os.makedirs(model_folder)\n","\n","    train_log_loss = open((model_folder + '/train_log_loss.txt'), 'w')\n","    train_log_acc = open((model_folder + '/train_log_acc.txt'), 'w')\n","    val_log_loss = open((model_folder + '/val_log_loss.txt'), 'w')\n","    val_log_acc = open((model_folder + '/val_log_acc.txt'), 'w')\n","\n","    # Train val partitioning\n","    train_usr = [\"S1\", \"S3\", \"S4\"]\n","    val_usr = [\"S2\"]\n","\n","    # Data loader\n","    normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    spatial_transform = Compose(\n","        [Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n","         ToTensor(), normalize])\n","\n","    vid_seq_train = makeDataset(train_data_dir, train_usr,\n","                                spatial_transform=spatial_transform, seqLen=seqLen)\n","\n","    train_loader = torch.utils.data.DataLoader(vid_seq_train, batch_size=trainBatchSize,\n","                                               shuffle=True, num_workers=4, pin_memory=True)\n","\n","    vid_seq_val = makeDataset(train_data_dir, val_usr,\n","                              spatial_transform=Compose([Scale(256), CenterCrop(224), ToTensor(), normalize]),\n","                              seqLen=seqLen, phase=\"test\")\n","\n","    val_loader = torch.utils.data.DataLoader(vid_seq_val, batch_size=valBatchSize,\n","                                             shuffle=False, num_workers=2, pin_memory=True)\n","\n","    train_params = []\n","\n","    # stage 1: train only lstm\n","    if stage == 1:\n","\n","        model = attentionModel(num_classes=num_classes, mem_size=mem_size)\n","        model.train(False)\n","        for params in model.parameters():\n","            params.requires_grad = False\n","\n","    # stage 2: train lstm, layer4, spatial attention and final fc\n","    else:\n","        model = attentionModel(num_classes=num_classes, mem_size=mem_size)\n","        model.load_state_dict(torch.load(stage1_dict), strict=False)  # pretrained\n","        model.train(False)\n","        for params in model.parameters():\n","            params.requires_grad = False\n","        #\n","        for params in model.resNet.layer4[0].conv1.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        for params in model.resNet.layer4[0].conv2.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        for params in model.resNet.layer4[1].conv1.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        for params in model.resNet.layer4[1].conv2.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        for params in model.resNet.layer4[2].conv1.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","        #\n","        for params in model.resNet.layer4[2].conv2.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","        #\n","        for params in model.resNet.fc.parameters():  # fully connected layer\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","        model.resNet.layer4[0].conv1.train(True)\n","        model.resNet.layer4[0].conv2.train(True)\n","        model.resNet.layer4[1].conv1.train(True)\n","        model.resNet.layer4[1].conv2.train(True)\n","        model.resNet.layer4[2].conv1.train(True)\n","        model.resNet.layer4[2].conv2.train(True)\n","        model.resNet.fc.train(True)\n","\n","        # set to train the self supervised\n","        for params in model.ms_task.parameters():\n","            params.requires_grad = True\n","            train_params += [params]\n","\n","    for params in model.lstm_cell.parameters():  # for both stages we train the lstm\n","        params.requires_grad = True\n","        train_params += [params]\n","\n","    for params in model.classifier.parameters():  # for both stages we train the last classifier (after the lstm and avg pooling)\n","        params.requires_grad = True\n","        train_params += [params]\n","\n","    \n","\n","    model.lstm_cell.train(True)\n","\n","    model.classifier.train(True)\n","    model.cuda()\n","\n","    loss_fn = nn.CrossEntropyLoss()\n","\n","    optimizer_fn = torch.optim.Adam(train_params, lr=lr1, weight_decay=4e-5, eps=1e-4)\n","\n","    optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=decay_step,\n","                                                           gamma=decay_factor)\n","\n","    train_iter = 0\n","    min_accuracy = 0\n","    \n","    for epoch in range(numEpochs):\n","        optim_scheduler.step()\n","        epoch_loss = 0\n","        epoch_ms_loss = 0\n","        numCorrTrain = 0\n","        trainSamples = 0\n","        iterPerEpoch = 0\n","        model.lstm_cell.train(True)\n","        model.classifier.train(True)\n","        if stage == 2:\n","            model.resNet.layer4[0].conv1.train(True)\n","            model.resNet.layer4[0].conv2.train(True)\n","            model.resNet.layer4[1].conv1.train(True)\n","            model.resNet.layer4[1].conv2.train(True)\n","            model.resNet.layer4[2].conv1.train(True)\n","            model.resNet.layer4[2].conv2.train(True)\n","            model.resNet.fc.train(True)\n","\n","            model.ms_task.conv.train(True)\n","            model.ms_task.fc.train(True)\n","\n","        start = time.time()\n","        for i, (inputs, labelMaps, targets) in enumerate(train_loader):\n","            train_iter += 1\n","            iterPerEpoch += 1\n","            optimizer_fn.zero_grad()\n","            inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).to(DEVICE))\n","            labelVariable = Variable(targets.to(DEVICE))\n","            trainSamples += inputs.size(0)\n","            output_label, _, ms_lab = model(inputVariable, stage)\n","\n","            loss = loss_fn(output_label, labelVariable)\n","\n","            if stage==2:\n","                ms_loss = loss_ms_fn(F.softmax(ms_lab, dim=1), mmaps_preparation(labelMaps, 1e-2))\n","                epoch_ms_loss += ms_loss.data.item() #\n","                (loss+ms_loss).backward()\n","            else:\n","                loss.backward()\n","\n","            optimizer_fn.step()\n","            _, predicted = torch.max(output_label.data, 1)\n","            numCorrTrain += (predicted == targets.to(DEVICE)).sum()  # evaluating number of correct classifications\n","            epoch_loss += loss.data.item()\n","        print(f\"Elapsed {time.time()-start}\")\n","\n","        avg_loss = epoch_loss / iterPerEpoch\n","        if stage==2:\n","            avg_ms_loss = epoch_ms_loss / iterPerEpoch #\n","\n","        trainAccuracy = (numCorrTrain.data.item() / trainSamples)\n","\n","        train_log_loss.write('Training loss after {} epoch = {}\\n'.format(epoch + 1, avg_loss))  # log file\n","        train_log_acc.write('Training accuracy after {} epoch = {}\\n'.format(epoch + 1, trainAccuracy))  # log file\n","        print('Train: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch + 1, avg_loss, trainAccuracy))\n","        if stage == 2:\n","            train_log_loss.write('Training ms loss after {} epoch = {}\\n'.format(epoch + 1, avg_ms_loss))  # log file\n","            print(\"ms avg epoch loss: {}\".format(avg_ms_loss))\n","\n","        if (epoch + 1) % VAL_FREQUENCY == 0:\n","            model.train(False)\n","            val_loss_epoch = 0\n","            val_ms_loss_epoch = 0\n","            val_iter = 0\n","            val_samples = 0\n","            numCorr = 0\n","            for j, (inputs, labelMaps, targets) in enumerate(val_loader):\n","                val_iter += 1\n","                val_samples += inputs.size(0)\n","                inputVariable = Variable(inputs.permute(1, 0, 2, 3, 4).to(DEVICE))\n","                labelVariable = Variable(targets.to(DEVICE))\n","                output_label, _, ms_lab = model(inputVariable, stage)\n","                val_loss = loss_fn(output_label, labelVariable)\n","                val_loss_epoch += val_loss.data.item()\n","\n","                if stage==2:\n","                    ms_loss = loss_ms_fn(F.softmax(ms_lab, dim=1), mmaps_preparation(labelMaps, 1e-2)) #\n","                    val_ms_loss_epoch += ms_loss.data.item() #\n","\n","\n","                _, predicted = torch.max(output_label.data, 1)\n","                numCorr += (predicted == targets.to(DEVICE)).sum()  # evaluating number of correct classifications\n","            val_accuracy = (numCorr.data.item() / val_samples)\n","            avg_val_loss = val_loss_epoch / val_iter\n","            avg_ms_loss = val_ms_loss_epoch / val_iter  #\n","\n","            print('Val: Epoch = {} | Loss {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n","            val_log_loss.write('Val Loss after {} epochs = {}\\n'.format(epoch + 1, avg_val_loss))  # log file\n","            val_log_acc.write('Val Accuracy after {} epochs = {}%\\n'.format(epoch + 1, val_accuracy))  # log file\n","            \n","            if stage == 2:\n","                val_log_loss.write('Val ms loss after {} epoch = {}\\n'.format(epoch + 1, avg_ms_loss))  # log file\n","                print(\"Ms Epoch: {}\".format(avg_ms_loss))\n","\n","            if val_accuracy > min_accuracy:\n","                save_path_model = (\n","                        model_folder + '/model_rgb_state_dict.pth')  # every epoch, check if the val accuracy is improved, if so, save that model\n","                torch.save(model.state_dict(),\n","                           save_path_model)  # in that way, even if the model overfit, you will get always the best model\n","                min_accuracy = val_accuracy  # in this way you don't have to care too much about the number of epochs\n","\n","    train_log_loss.close()\n","    train_log_acc.close()\n","    val_log_acc.close()\n","    val_log_loss.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BRd39-S8g4Yz","colab_type":"code","colab":{}},"source":["def __main__():\n","    version = \"rgb_16frames_regression_kl\"\n","    trainDatasetDir = \"/content/\"\n","    outDir = \"results\"\n","    #stage1Dict = \"./\" + outDir + \"/\" + version + \"_1/model_rgb_state_dict.pth\"  # args.stage1Dict\n","    stage1Dict = \"/content/drive/My Drive/FINAL_LOGS/200+150epochs_RGB_16frames/test_1/model_rgb_state_dict.pth\"\n","\n","    # STAGE 1 PARAMETERS\n","    ST1_seqLen = 16  # 7\n","    ST1_trainBatchSize = 32  # 32\n","    ST1_valBatchSize = 32  # 32\n","    ST1_numEpochs = 200  # 200\n","    ST1_lr1 = 1e-3  # 1e-3\n","    ST1_stepSize = [25, 75, 150]  # [25, 75, 150]\n","    ST1_decayRate = 0.1  # 0.1\n","    ST1_memSize = 512  # 512\n","\n","    # STAGE 2 PARAMETERS\n","    ST2_seqLen = 16  # 7\n","    ST2_trainBatchSize = 16  # 32\n","    ST2_valBatchSize = 16  # 32\n","    ST2_numEpochs = 150  # 150\n","    ST2_lr1 = 1e-4  # 1e-4\n","    ST2_stepSize = [50, 100]  # [25, 75]\n","    ST2_decayRate = 0.1  # 0.1\n","    ST2_memSize = 512  # 512\n","\n","    # STAGE 1\n","    '''\n","    main_run(version + \"_1\",\n","             stage=1,\n","             train_data_dir=trainDatasetDir,\n","             stage1_dict=stage1Dict,\n","             out_dir=outDir,\n","             seqLen=ST1_seqLen,\n","             trainBatchSize=ST1_trainBatchSize,\n","             valBatchSize=ST1_valBatchSize,\n","             numEpochs=ST1_numEpochs,\n","             lr1=ST1_lr1,\n","             decay_factor=ST1_decayRate,\n","             decay_step=ST1_stepSize,\n","             mem_size=ST1_memSize)\n","    '''   \n","    # STAGE 2\n","    main_run(version + \"_2\",\n","             stage=2,\n","             train_data_dir=trainDatasetDir,\n","             stage1_dict=stage1Dict,\n","             out_dir=outDir,\n","             seqLen=ST2_seqLen,\n","             trainBatchSize=ST2_trainBatchSize,\n","             valBatchSize=ST2_valBatchSize,\n","             numEpochs=ST2_numEpochs,\n","             lr1=ST2_lr1,\n","             decay_factor=ST2_decayRate,\n","             decay_step=ST2_stepSize,\n","             mem_size=ST2_memSize)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z4eZN7d5i2qy","colab_type":"code","colab":{}},"source":["#!rm \"-r\" \"./results/rgb_16frames_2\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ymmr3V-Eg_fo","colab_type":"code","outputId":"23c00b17-fe3d-45d6-e04a-7f4a42e9137c","executionInfo":{"status":"ok","timestamp":1591537298202,"user_tz":-120,"elapsed":7371455,"user":{"displayName":"Laboratorio MLDL","photoUrl":"","userId":"15250599834567100244"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["__main__()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["skipped /content/processed_frames/S1/pour_sugar,spoon,cup/2, different frame number\n","skipped /content/processed_frames/S1/take_honey/3, different frame number\n","skipped /content/processed_frames/S1/take_peanut/1, different frame number\n","skipped /content/processed_frames/S3/pour_coffee,spoon,cup/1, different frame number\n","skipped /content/processed_frames/S3/pour_coffee,spoon,cup/3, different frame number\n","skipped /content/processed_frames/S3/pour_sugar,spoon,cup/1, different frame number\n","skipped /content/processed_frames/S3/pour_sugar,spoon,cup/2, different frame number\n","skipped /content/processed_frames/S3/pour_sugar,spoon,cup/4, different frame number\n","skipped /content/processed_frames/S3/stir_spoon,cup/2, different frame number\n","skipped /content/processed_frames/S4/pour_coffee,spoon,cup/1, different frame number\n","skipped /content/processed_frames/S4/pour_coffee,spoon,cup/2, different frame number\n","skipped /content/processed_frames/S4/pour_coffee,spoon,cup/3, different frame number\n","skipped /content/processed_frames/S4/pour_coffee,spoon,cup/4, different frame number\n","skipped /content/processed_frames/S4/pour_sugar,spoon,cup/1, different frame number\n","skipped /content/processed_frames/S4/pour_sugar,spoon,cup/2, different frame number\n","skipped /content/processed_frames/S4/pour_sugar,spoon,cup/3, different frame number\n","skipped /content/processed_frames/S2/pour_coffee,spoon,cup/2, different frame number\n","skipped /content/processed_frames/S2/take_mustard/1, different frame number\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:31: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_i_xx.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:32: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  torch.nn.init.constant(self.conv_i_xx.bias, 0)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:33: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_i_hh.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:35: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_f_xx.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:36: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  torch.nn.init.constant(self.conv_f_xx.bias, 0)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:37: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_f_hh.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:39: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_c_xx.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  torch.nn.init.constant(self.conv_c_xx.bias, 0)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:41: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_c_hh.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:43: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_o_xx.weight)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:44: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n","  torch.nn.init.constant(self.conv_o_xx.bias, 0)\n","/content/drive/My Drive/Lorenzo/ego-rnn-ss-task/MyConvLSTMCell.py:45: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n","  torch.nn.init.xavier_normal(self.conv_o_hh.weight)\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode))\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2247: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n","  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"],"name":"stderr"},{"output_type":"stream","text":["MS loss: 0.052379\n","MS loss: 0.065437\n","MS loss: 0.116285\n","MS loss: 0.082648\n","MS loss: 0.082951\n","MS loss: 0.088934\n","MS loss: 0.073495\n","MS loss: 0.092965\n","MS loss: 0.099172\n","MS loss: 0.070856\n","MS loss: 0.068892\n","MS loss: 0.118797\n","MS loss: 0.073909\n","MS loss: 0.082276\n","MS loss: 0.074237\n","MS loss: 0.081558\n","MS loss: 0.066172\n","MS loss: 0.082897\n","MS loss: 0.076410\n","MS loss: 0.087741\n","MS loss: 0.078940\n","Elapsed 49.07799029350281\n","Train: Epoch = 1 | Loss = 2.4373954364231656 | Accuracy = 0.3476923076923077\n","ms avg epoch loss: 0.08175951118270557\n","MS loss: 0.086460\n","MS loss: 0.089040\n","MS loss: 0.076985\n","MS loss: 0.101292\n","MS loss: 0.106754\n","MS loss: 0.092884\n","MS loss: 0.062613\n","MS loss: 0.063575\n","MS loss: 0.073074\n","MS loss: 0.085078\n","MS loss: 0.096871\n","MS loss: 0.082176\n","MS loss: 0.098688\n","MS loss: 0.071177\n","MS loss: 0.069381\n","MS loss: 0.082008\n","MS loss: 0.041951\n","MS loss: 0.088078\n","MS loss: 0.080645\n","MS loss: 0.079366\n","MS loss: 0.082980\n","Elapsed 48.840630292892456\n","Train: Epoch = 2 | Loss = 2.448487259092785 | Accuracy = 0.3169230769230769\n","ms avg epoch loss: 0.08147975766942614\n","MS loss: 0.100299\n","MS loss: 0.081624\n","MS loss: 0.078406\n","MS loss: 0.094937\n","MS loss: 0.092403\n","MS loss: 0.053067\n","MS loss: 0.059753\n","MS loss: 0.069075\n","MS loss: 0.063507\n","MS loss: 0.070830\n","MS loss: 0.062037\n","MS loss: 0.076017\n","MS loss: 0.078610\n","MS loss: 0.058116\n","MS loss: 0.075378\n","MS loss: 0.096589\n","MS loss: 0.085908\n","MS loss: 0.100748\n","MS loss: 0.074253\n","MS loss: 0.062258\n","MS loss: 0.080486\n","Elapsed 48.76806926727295\n","Train: Epoch = 3 | Loss = 2.341458485240028 | Accuracy = 0.3569230769230769\n","ms avg epoch loss: 0.07687142775172279\n","MS loss: 0.096699\n","MS loss: 0.088724\n","MS loss: 0.072098\n","MS loss: 0.105220\n","MS loss: 0.082076\n","MS loss: 0.076545\n","MS loss: 0.106955\n","MS loss: 0.100081\n","MS loss: 0.055656\n","MS loss: 0.058433\n","MS loss: 0.056438\n","MS loss: 0.073612\n","MS loss: 0.101675\n","MS loss: 0.099085\n","MS loss: 0.087957\n","MS loss: 0.118082\n","MS loss: 0.082954\n","MS loss: 0.055177\n","MS loss: 0.113325\n","MS loss: 0.096131\n","MS loss: 0.064759\n","Elapsed 47.69698786735535\n","Train: Epoch = 4 | Loss = 2.060857199487232 | Accuracy = 0.40923076923076923\n","ms avg epoch loss: 0.0853181660530113\n","MS loss: 0.070281\n","MS loss: 0.059780\n","MS loss: 0.080419\n","MS loss: 0.079560\n","MS loss: 0.047418\n","MS loss: 0.088261\n","MS loss: 0.087310\n","MS loss: 0.063045\n","MS loss: 0.069576\n","MS loss: 0.107076\n","MS loss: 0.061140\n","MS loss: 0.063141\n","MS loss: 0.088451\n","MS loss: 0.097132\n","MS loss: 0.062395\n","MS loss: 0.072070\n","MS loss: 0.091628\n","MS loss: 0.099661\n","MS loss: 0.077955\n","MS loss: 0.079465\n","MS loss: 0.069058\n","Elapsed 47.302847146987915\n","Train: Epoch = 5 | Loss = 2.243311393828619 | Accuracy = 0.3723076923076923\n","ms avg epoch loss: 0.07689639784040905\n","MS loss: 0.147426\n","MS loss: 0.180926\n","MS loss: 0.108485\n","MS loss: 0.096990\n","MS loss: 0.097378\n","MS loss: 0.090864\n","MS loss: 0.078226\n","MS loss: 0.072388\n","Val: Epoch = 5 | Loss 2.154267057776451 | Accuracy = 0.2982456140350877\n","Ms Epoch: 0.10908543225377798\n","MS loss: 0.091075\n","MS loss: 0.086853\n","MS loss: 0.079042\n","MS loss: 0.098699\n","MS loss: 0.108120\n","MS loss: 0.078205\n","MS loss: 0.070796\n","MS loss: 0.068412\n","MS loss: 0.073105\n","MS loss: 0.079246\n","MS loss: 0.096911\n","MS loss: 0.075781\n","MS loss: 0.090277\n","MS loss: 0.109449\n","MS loss: 0.079410\n","MS loss: 0.074869\n","MS loss: 0.059169\n","MS loss: 0.129826\n","MS loss: 0.081257\n","MS loss: 0.109772\n","MS loss: 0.088638\n","Elapsed 48.24865961074829\n","Train: Epoch = 6 | Loss = 2.237578238759722 | Accuracy = 0.39076923076923076\n","ms avg epoch loss: 0.08709110018043291\n","MS loss: 0.080794\n","MS loss: 0.030921\n","MS loss: 0.089636\n","MS loss: 0.068385\n","MS loss: 0.079818\n","MS loss: 0.040817\n","MS loss: 0.077933\n","MS loss: 0.092840\n","MS loss: 0.076470\n","MS loss: 0.084193\n","MS loss: 0.096930\n","MS loss: 0.088747\n","MS loss: 0.089770\n","MS loss: 0.077918\n","MS loss: 0.080338\n","MS loss: 0.066394\n","MS loss: 0.075963\n","MS loss: 0.071487\n","MS loss: 0.091683\n","MS loss: 0.092774\n","MS loss: 0.091288\n","Elapsed 47.27659749984741\n","Train: Epoch = 7 | Loss = 2.1643837803886052 | Accuracy = 0.39384615384615385\n","ms avg epoch loss: 0.07833803303184964\n","MS loss: 0.082678\n","MS loss: 0.101264\n","MS loss: 0.057895\n","MS loss: 0.109259\n","MS loss: 0.077804\n","MS loss: 0.086116\n","MS loss: 0.063395\n","MS loss: 0.100428\n","MS loss: 0.101366\n","MS loss: 0.076195\n","MS loss: 0.094485\n","MS loss: 0.063473\n","MS loss: 0.058572\n","MS loss: 0.111728\n","MS loss: 0.052902\n","MS loss: 0.103383\n","MS loss: 0.100398\n","MS loss: 0.096833\n","MS loss: 0.094577\n","MS loss: 0.101858\n","MS loss: 0.096805\n","Elapsed 47.97863745689392\n","Train: Epoch = 8 | Loss = 2.091761736642747 | Accuracy = 0.38461538461538464\n","ms avg epoch loss: 0.08721007566366877\n","MS loss: 0.079788\n","MS loss: 0.076231\n","MS loss: 0.071225\n","MS loss: 0.080715\n","MS loss: 0.082420\n","MS loss: 0.072290\n","MS loss: 0.054959\n","MS loss: 0.084472\n","MS loss: 0.095808\n","MS loss: 0.103462\n","MS loss: 0.062880\n","MS loss: 0.083721\n","MS loss: 0.119381\n","MS loss: 0.084904\n","MS loss: 0.055215\n","MS loss: 0.079394\n","MS loss: 0.070840\n","MS loss: 0.054212\n","MS loss: 0.072210\n","MS loss: 0.072493\n","MS loss: 0.053439\n","Elapsed 47.792279958724976\n","Train: Epoch = 9 | Loss = 2.197755830628531 | Accuracy = 0.3723076923076923\n","ms avg epoch loss: 0.07666948579606556\n","MS loss: 0.123275\n","MS loss: 0.036527\n","MS loss: 0.066284\n","MS loss: 0.087920\n","MS loss: 0.079627\n","MS loss: 0.041199\n","MS loss: 0.107599\n","MS loss: 0.075216\n","MS loss: 0.086912\n","MS loss: 0.075316\n","MS loss: 0.063710\n","MS loss: 0.110970\n","MS loss: 0.083943\n","MS loss: 0.059535\n","MS loss: 0.068114\n","MS loss: 0.095136\n","MS loss: 0.059918\n","MS loss: 0.059333\n","MS loss: 0.094542\n","MS loss: 0.067576\n","MS loss: 0.138754\n","Elapsed 47.81965446472168\n","Train: Epoch = 10 | Loss = 2.127670929545448 | Accuracy = 0.38153846153846155\n","ms avg epoch loss: 0.08006701192685536\n","MS loss: 0.147441\n","MS loss: 0.180908\n","MS loss: 0.108478\n","MS loss: 0.096979\n","MS loss: 0.097367\n","MS loss: 0.090863\n","MS loss: 0.078221\n","MS loss: 0.072362\n","Val: Epoch = 10 | Loss 1.8466941863298416 | Accuracy = 0.4298245614035088\n","Ms Epoch: 0.1090773856267333\n","MS loss: 0.070461\n","MS loss: 0.075271\n","MS loss: 0.051192\n","MS loss: 0.068425\n","MS loss: 0.080582\n","MS loss: 0.134961\n","MS loss: 0.068433\n","MS loss: 0.102947\n","MS loss: 0.051110\n","MS loss: 0.052744\n","MS loss: 0.112345\n","MS loss: 0.092774\n","MS loss: 0.063391\n","MS loss: 0.059849\n","MS loss: 0.055639\n","MS loss: 0.073518\n","MS loss: 0.065846\n","MS loss: 0.073576\n","MS loss: 0.090496\n","MS loss: 0.089888\n","MS loss: 0.140426\n","Elapsed 48.10485219955444\n","Train: Epoch = 11 | Loss = 1.9371296962102253 | Accuracy = 0.47692307692307695\n","ms avg epoch loss: 0.07970822637989408\n","MS loss: 0.095077\n","MS loss: 0.055439\n","MS loss: 0.101332\n","MS loss: 0.095842\n","MS loss: 0.076504\n","MS loss: 0.068649\n","MS loss: 0.092344\n","MS loss: 0.086820\n","MS loss: 0.086582\n","MS loss: 0.085997\n","MS loss: 0.101010\n","MS loss: 0.092208\n","MS loss: 0.068738\n","MS loss: 0.068989\n","MS loss: 0.056162\n","MS loss: 0.128874\n","MS loss: 0.077493\n","MS loss: 0.077571\n","MS loss: 0.100443\n","MS loss: 0.098903\n","MS loss: 0.077584\n","Elapsed 47.89132261276245\n","Train: Epoch = 12 | Loss = 1.963004076764697 | Accuracy = 0.40615384615384614\n","ms avg epoch loss: 0.08536000532053765\n","MS loss: 0.066270\n","MS loss: 0.090575\n","MS loss: 0.060209\n","MS loss: 0.062168\n","MS loss: 0.055691\n","MS loss: 0.057473\n","MS loss: 0.053591\n","MS loss: 0.116922\n","MS loss: 0.074969\n","MS loss: 0.097790\n","MS loss: 0.075667\n","MS loss: 0.067836\n","MS loss: 0.115601\n","MS loss: 0.094892\n","MS loss: 0.091406\n","MS loss: 0.102158\n","MS loss: 0.067470\n","MS loss: 0.071178\n","MS loss: 0.066944\n","MS loss: 0.053422\n","MS loss: 0.077317\n","Elapsed 48.112324476242065\n","Train: Epoch = 13 | Loss = 1.8385087166513716 | Accuracy = 0.4584615384615385\n","ms avg epoch loss: 0.0771213849740369\n","MS loss: 0.047787\n","MS loss: 0.095637\n","MS loss: 0.070887\n","MS loss: 0.098411\n","MS loss: 0.107325\n","MS loss: 0.062992\n","MS loss: 0.083641\n","MS loss: 0.090895\n","MS loss: 0.103832\n","MS loss: 0.084219\n","MS loss: 0.075979\n","MS loss: 0.052798\n","MS loss: 0.101741\n","MS loss: 0.072420\n","MS loss: 0.075049\n","MS loss: 0.088721\n","MS loss: 0.062298\n","MS loss: 0.094652\n","MS loss: 0.099277\n","MS loss: 0.095274\n","MS loss: 0.031198\n","Elapsed 47.78475546836853\n","Train: Epoch = 14 | Loss = 1.8687617097582137 | Accuracy = 0.4584615384615385\n","ms avg epoch loss: 0.08071594907059557\n","MS loss: 0.046837\n","MS loss: 0.081554\n","MS loss: 0.072743\n","MS loss: 0.074560\n","MS loss: 0.126337\n","MS loss: 0.127278\n","MS loss: 0.092347\n","MS loss: 0.079923\n","MS loss: 0.086886\n","MS loss: 0.083649\n","MS loss: 0.053581\n","MS loss: 0.085819\n","MS loss: 0.081434\n","MS loss: 0.070998\n","MS loss: 0.108848\n","MS loss: 0.076466\n","MS loss: 0.080993\n","MS loss: 0.083731\n","MS loss: 0.082900\n","MS loss: 0.092861\n","MS loss: 0.069375\n","Elapsed 48.244741439819336\n","Train: Epoch = 15 | Loss = 1.70679733866737 | Accuracy = 0.48923076923076925\n","ms avg epoch loss: 0.08376760213148027\n","MS loss: 0.147434\n","MS loss: 0.180898\n","MS loss: 0.108477\n","MS loss: 0.096968\n","MS loss: 0.097363\n","MS loss: 0.090860\n","MS loss: 0.078222\n","MS loss: 0.072322\n","Val: Epoch = 15 | Loss 1.9508837908506393 | Accuracy = 0.45614035087719296\n","Ms Epoch: 0.10906799603253603\n","MS loss: 0.073261\n","MS loss: 0.048765\n","MS loss: 0.075959\n","MS loss: 0.078525\n","MS loss: 0.094311\n","MS loss: 0.119938\n","MS loss: 0.088176\n","MS loss: 0.088231\n","MS loss: 0.070155\n","MS loss: 0.054940\n","MS loss: 0.081276\n","MS loss: 0.073292\n","MS loss: 0.056788\n","MS loss: 0.090312\n","MS loss: 0.074147\n","MS loss: 0.076789\n","MS loss: 0.085522\n","MS loss: 0.090311\n","MS loss: 0.042819\n","MS loss: 0.083787\n","MS loss: 0.025743\n","Elapsed 48.48107051849365\n","Train: Epoch = 16 | Loss = 1.8266279016222273 | Accuracy = 0.4646153846153846\n","ms avg epoch loss: 0.07490695906536919\n","MS loss: 0.087655\n","MS loss: 0.124676\n","MS loss: 0.051488\n","MS loss: 0.081147\n","MS loss: 0.039924\n","MS loss: 0.048985\n","MS loss: 0.082518\n","MS loss: 0.079125\n","MS loss: 0.048960\n","MS loss: 0.075205\n","MS loss: 0.104480\n","MS loss: 0.083309\n","MS loss: 0.070943\n","MS loss: 0.076569\n","MS loss: 0.099940\n","MS loss: 0.062074\n","MS loss: 0.047833\n","MS loss: 0.083310\n","MS loss: 0.089836\n","MS loss: 0.082030\n","MS loss: 0.178306\n","Elapsed 48.072866439819336\n","Train: Epoch = 17 | Loss = 1.8373993010748 | Accuracy = 0.4707692307692308\n","ms avg epoch loss: 0.0808719735415209\n","MS loss: 0.096431\n","MS loss: 0.118708\n","MS loss: 0.083183\n","MS loss: 0.075127\n","MS loss: 0.098776\n","MS loss: 0.091102\n","MS loss: 0.059674\n","MS loss: 0.069580\n","MS loss: 0.093532\n","MS loss: 0.073481\n","MS loss: 0.068323\n","MS loss: 0.138536\n","MS loss: 0.054840\n","MS loss: 0.111287\n","MS loss: 0.093755\n","MS loss: 0.103086\n","MS loss: 0.055248\n","MS loss: 0.069779\n","MS loss: 0.052314\n","MS loss: 0.062797\n","MS loss: 0.080368\n","Elapsed 48.482587575912476\n","Train: Epoch = 18 | Loss = 1.7649088956060863 | Accuracy = 0.5046153846153846\n","ms avg epoch loss: 0.08332980566081546\n","MS loss: 0.086717\n","MS loss: 0.062851\n","MS loss: 0.156110\n","MS loss: 0.102503\n","MS loss: 0.095499\n","MS loss: 0.100919\n","MS loss: 0.080979\n","MS loss: 0.069697\n","MS loss: 0.102438\n","MS loss: 0.073062\n","MS loss: 0.078217\n","MS loss: 0.052337\n","MS loss: 0.078184\n","MS loss: 0.064883\n","MS loss: 0.089001\n","MS loss: 0.117272\n","MS loss: 0.050086\n","MS loss: 0.085124\n","MS loss: 0.073270\n","MS loss: 0.071251\n","MS loss: 0.096861\n","Elapsed 48.38895130157471\n","Train: Epoch = 19 | Loss = 1.931326985359192 | Accuracy = 0.4584615384615385\n","ms avg epoch loss: 0.08510761619323776\n","MS loss: 0.091348\n","MS loss: 0.069114\n","MS loss: 0.096437\n","MS loss: 0.083014\n","MS loss: 0.075873\n","MS loss: 0.086101\n","MS loss: 0.090176\n","MS loss: 0.059234\n","MS loss: 0.111413\n","MS loss: 0.062720\n","MS loss: 0.080332\n","MS loss: 0.118771\n","MS loss: 0.087516\n","MS loss: 0.103794\n","MS loss: 0.101871\n","MS loss: 0.071556\n","MS loss: 0.064242\n","MS loss: 0.043969\n","MS loss: 0.064954\n","MS loss: 0.083199\n","MS loss: 0.061619\n","Elapsed 47.63122296333313\n","Train: Epoch = 20 | Loss = 1.8882505212511336 | Accuracy = 0.4461538461538462\n","ms avg epoch loss: 0.0812976819773515\n","MS loss: 0.147438\n","MS loss: 0.180894\n","MS loss: 0.108465\n","MS loss: 0.096973\n","MS loss: 0.097347\n","MS loss: 0.090856\n","MS loss: 0.078212\n","MS loss: 0.072340\n","Val: Epoch = 20 | Loss 1.3493805229663849 | Accuracy = 0.5789473684210527\n","Ms Epoch: 0.10906558390706778\n","MS loss: 0.070945\n","MS loss: 0.076028\n","MS loss: 0.049801\n","MS loss: 0.112549\n","MS loss: 0.126562\n","MS loss: 0.050361\n","MS loss: 0.056449\n","MS loss: 0.079719\n","MS loss: 0.095669\n","MS loss: 0.048253\n","MS loss: 0.091158\n","MS loss: 0.081348\n","MS loss: 0.066132\n","MS loss: 0.072479\n","MS loss: 0.057095\n","MS loss: 0.133969\n","MS loss: 0.112286\n","MS loss: 0.104836\n","MS loss: 0.086386\n","MS loss: 0.075955\n","MS loss: 0.029260\n","Elapsed 48.29657053947449\n","Train: Epoch = 21 | Loss = 1.6705227295557659 | Accuracy = 0.5138461538461538\n","ms avg epoch loss: 0.07986869130815778\n","MS loss: 0.091539\n","MS loss: 0.067502\n","MS loss: 0.084590\n","MS loss: 0.085980\n","MS loss: 0.048276\n","MS loss: 0.112361\n","MS loss: 0.106239\n","MS loss: 0.054594\n","MS loss: 0.074955\n","MS loss: 0.067458\n","MS loss: 0.099971\n","MS loss: 0.054701\n","MS loss: 0.098083\n","MS loss: 0.094381\n","MS loss: 0.071186\n","MS loss: 0.098877\n","MS loss: 0.088272\n","MS loss: 0.059976\n","MS loss: 0.107655\n","MS loss: 0.075543\n","MS loss: 0.085888\n","Elapsed 47.6774218082428\n","Train: Epoch = 22 | Loss = 1.745364779517764 | Accuracy = 0.4707692307692308\n","ms avg epoch loss: 0.08228696794027374\n","MS loss: 0.070916\n","MS loss: 0.099276\n","MS loss: 0.058278\n","MS loss: 0.089839\n","MS loss: 0.083319\n","MS loss: 0.079655\n","MS loss: 0.090405\n","MS loss: 0.040445\n","MS loss: 0.100865\n","MS loss: 0.078614\n","MS loss: 0.094137\n","MS loss: 0.104325\n","MS loss: 0.094047\n","MS loss: 0.062562\n","MS loss: 0.072216\n","MS loss: 0.084081\n","MS loss: 0.039188\n","MS loss: 0.090867\n","MS loss: 0.091958\n","MS loss: 0.069158\n","MS loss: 0.099449\n","Elapsed 47.84390187263489\n","Train: Epoch = 23 | Loss = 1.6663853384199596 | Accuracy = 0.5138461538461538\n","ms avg epoch loss: 0.08064763797890573\n","MS loss: 0.069493\n","MS loss: 0.074978\n","MS loss: 0.076991\n","MS loss: 0.074517\n","MS loss: 0.080203\n","MS loss: 0.116831\n","MS loss: 0.077100\n","MS loss: 0.093659\n","MS loss: 0.114342\n","MS loss: 0.054419\n","MS loss: 0.088309\n","MS loss: 0.075003\n","MS loss: 0.066854\n","MS loss: 0.062762\n","MS loss: 0.087164\n","MS loss: 0.106447\n","MS loss: 0.120381\n","MS loss: 0.112413\n","MS loss: 0.075001\n","MS loss: 0.057019\n","MS loss: 0.076079\n","Elapsed 48.2303671836853\n","Train: Epoch = 24 | Loss = 1.7091076090222312 | Accuracy = 0.5015384615384615\n","ms avg epoch loss: 0.08380779872337978\n","MS loss: 0.073933\n","MS loss: 0.069314\n","MS loss: 0.059698\n","MS loss: 0.110238\n","MS loss: 0.112109\n","MS loss: 0.080683\n","MS loss: 0.107825\n","MS loss: 0.077078\n","MS loss: 0.062222\n","MS loss: 0.064710\n","MS loss: 0.066532\n","MS loss: 0.094066\n","MS loss: 0.112078\n","MS loss: 0.085314\n","MS loss: 0.066978\n","MS loss: 0.086076\n","MS loss: 0.072997\n","MS loss: 0.091895\n","MS loss: 0.119202\n","MS loss: 0.084136\n","MS loss: 0.110117\n","Elapsed 48.043156147003174\n","Train: Epoch = 25 | Loss = 1.745061891419547 | Accuracy = 0.52\n","ms avg epoch loss: 0.0860572208960851\n","MS loss: 0.147423\n","MS loss: 0.180899\n","MS loss: 0.108477\n","MS loss: 0.096970\n","MS loss: 0.097350\n","MS loss: 0.090867\n","MS loss: 0.078208\n","MS loss: 0.072315\n","Val: Epoch = 25 | Loss 1.5014981776475906 | Accuracy = 0.4824561403508772\n","Ms Epoch: 0.109063521027565\n","MS loss: 0.113095\n","MS loss: 0.092720\n","MS loss: 0.071481\n","MS loss: 0.094660\n","MS loss: 0.094223\n","MS loss: 0.073742\n","MS loss: 0.082739\n","MS loss: 0.047927\n","MS loss: 0.079231\n","MS loss: 0.064492\n","MS loss: 0.049599\n","MS loss: 0.066781\n","MS loss: 0.083277\n","MS loss: 0.082834\n","MS loss: 0.084613\n","MS loss: 0.071549\n","MS loss: 0.061846\n","MS loss: 0.086538\n","MS loss: 0.076850\n","MS loss: 0.067705\n","MS loss: 0.033063\n","Elapsed 47.56481218338013\n","Train: Epoch = 26 | Loss = 1.6894446668170748 | Accuracy = 0.5384615384615384\n","ms avg epoch loss: 0.07518867988671575\n","MS loss: 0.087058\n","MS loss: 0.087813\n","MS loss: 0.078507\n","MS loss: 0.088253\n","MS loss: 0.083564\n","MS loss: 0.067649\n","MS loss: 0.063777\n","MS loss: 0.134165\n","MS loss: 0.088531\n","MS loss: 0.099852\n","MS loss: 0.072674\n","MS loss: 0.090985\n","MS loss: 0.072562\n","MS loss: 0.082645\n","MS loss: 0.058704\n","MS loss: 0.095401\n","MS loss: 0.075144\n","MS loss: 0.050512\n","MS loss: 0.073917\n","MS loss: 0.075829\n","MS loss: 0.060736\n","Elapsed 48.29859733581543\n","Train: Epoch = 27 | Loss = 1.5536207244509743 | Accuracy = 0.5353846153846153\n","ms avg epoch loss: 0.08039431256197747\n","MS loss: 0.066304\n","MS loss: 0.082477\n","MS loss: 0.076307\n","MS loss: 0.080810\n","MS loss: 0.100975\n","MS loss: 0.049840\n","MS loss: 0.082150\n","MS loss: 0.092168\n","MS loss: 0.082512\n","MS loss: 0.049020\n","MS loss: 0.062700\n","MS loss: 0.062763\n","MS loss: 0.051843\n","MS loss: 0.095443\n","MS loss: 0.076368\n","MS loss: 0.061120\n","MS loss: 0.125369\n","MS loss: 0.064213\n","MS loss: 0.119920\n","MS loss: 0.094738\n","MS loss: 0.117155\n","Elapsed 47.52905988693237\n","Train: Epoch = 28 | Loss = 1.6819921533266704 | Accuracy = 0.49538461538461537\n","ms avg epoch loss: 0.08067606425001508\n","MS loss: 0.076705\n","MS loss: 0.070326\n","MS loss: 0.087407\n","MS loss: 0.065834\n","MS loss: 0.066675\n","MS loss: 0.120914\n","MS loss: 0.089573\n","MS loss: 0.083635\n","MS loss: 0.098722\n","MS loss: 0.066208\n","MS loss: 0.058384\n","MS loss: 0.090964\n","MS loss: 0.090800\n","MS loss: 0.056441\n","MS loss: 0.094825\n","MS loss: 0.077206\n","MS loss: 0.076847\n","MS loss: 0.095119\n","MS loss: 0.097092\n","MS loss: 0.070973\n","MS loss: 0.085200\n","Elapsed 48.220768213272095\n","Train: Epoch = 29 | Loss = 1.6345883380799067 | Accuracy = 0.5446153846153846\n","ms avg epoch loss: 0.08189768415121805\n","MS loss: 0.105016\n","MS loss: 0.041242\n","MS loss: 0.105913\n","MS loss: 0.069157\n","MS loss: 0.063419\n","MS loss: 0.081364\n","MS loss: 0.081404\n","MS loss: 0.068200\n","MS loss: 0.103659\n","MS loss: 0.072518\n","MS loss: 0.089474\n","MS loss: 0.062560\n","MS loss: 0.096101\n","MS loss: 0.084032\n","MS loss: 0.071074\n","MS loss: 0.081391\n","MS loss: 0.083148\n","MS loss: 0.078394\n","MS loss: 0.095220\n","MS loss: 0.098138\n","MS loss: 0.076413\n","Elapsed 48.22720909118652\n","Train: Epoch = 30 | Loss = 1.6271152922085352 | Accuracy = 0.5107692307692308\n","ms avg epoch loss: 0.08132558688521385\n","MS loss: 0.147439\n","MS loss: 0.180894\n","MS loss: 0.108472\n","MS loss: 0.096958\n","MS loss: 0.097354\n","MS loss: 0.090858\n","MS loss: 0.078216\n","MS loss: 0.072298\n","Val: Epoch = 30 | Loss 1.6357523798942566 | Accuracy = 0.5087719298245614\n","Ms Epoch: 0.10906099621206522\n","MS loss: 0.069847\n","MS loss: 0.092961\n","MS loss: 0.052302\n","MS loss: 0.066908\n","MS loss: 0.043636\n","MS loss: 0.070705\n","MS loss: 0.114780\n","MS loss: 0.073121\n","MS loss: 0.073182\n","MS loss: 0.081406\n","MS loss: 0.071482\n","MS loss: 0.066378\n","MS loss: 0.068254\n","MS loss: 0.067132\n","MS loss: 0.087004\n","MS loss: 0.064259\n","MS loss: 0.075107\n","MS loss: 0.136280\n","MS loss: 0.084558\n","MS loss: 0.057437\n","MS loss: 0.050919\n","Elapsed 47.95455551147461\n","Train: Epoch = 31 | Loss = 1.3949106704621088 | Accuracy = 0.56\n","ms avg epoch loss: 0.07465034936155591\n","MS loss: 0.056579\n","MS loss: 0.065862\n","MS loss: 0.062972\n","MS loss: 0.067711\n","MS loss: 0.063384\n","MS loss: 0.082835\n","MS loss: 0.075574\n","MS loss: 0.072272\n","MS loss: 0.108940\n","MS loss: 0.081985\n","MS loss: 0.095903\n","MS loss: 0.096452\n","MS loss: 0.056386\n","MS loss: 0.104955\n","MS loss: 0.121829\n","MS loss: 0.055351\n","MS loss: 0.083181\n","MS loss: 0.074075\n","MS loss: 0.148186\n","MS loss: 0.092168\n","MS loss: 0.031849\n","Elapsed 47.81815433502197\n","Train: Epoch = 32 | Loss = 1.6208486528623671 | Accuracy = 0.5415384615384615\n","ms avg epoch loss: 0.0808786405693917\n","MS loss: 0.088413\n","MS loss: 0.097864\n","MS loss: 0.092237\n","MS loss: 0.089250\n","MS loss: 0.063452\n","MS loss: 0.087013\n","MS loss: 0.077366\n","MS loss: 0.063140\n","MS loss: 0.062350\n","MS loss: 0.112197\n","MS loss: 0.091894\n","MS loss: 0.067861\n","MS loss: 0.110115\n","MS loss: 0.080922\n","MS loss: 0.058064\n","MS loss: 0.062028\n","MS loss: 0.091067\n","MS loss: 0.134517\n","MS loss: 0.066604\n","MS loss: 0.110417\n","MS loss: 0.073771\n","Elapsed 47.97118020057678\n","Train: Epoch = 33 | Loss = 1.6697770073300315 | Accuracy = 0.5261538461538462\n","ms avg epoch loss: 0.08478785856139093\n","MS loss: 0.067763\n","MS loss: 0.098034\n","MS loss: 0.053708\n","MS loss: 0.070069\n","MS loss: 0.076530\n","MS loss: 0.103734\n","MS loss: 0.070278\n","MS loss: 0.147865\n","MS loss: 0.092113\n","MS loss: 0.071687\n","MS loss: 0.095884\n","MS loss: 0.093578\n","MS loss: 0.065775\n","MS loss: 0.086609\n","MS loss: 0.121561\n","MS loss: 0.061447\n","MS loss: 0.070168\n","MS loss: 0.038460\n","MS loss: 0.099742\n","MS loss: 0.047988\n","MS loss: 0.142762\n","Elapsed 47.753194093704224\n","Train: Epoch = 34 | Loss = 1.5881900844119845 | Accuracy = 0.5261538461538462\n","ms avg epoch loss: 0.08455982396290415\n","MS loss: 0.111367\n","MS loss: 0.070255\n","MS loss: 0.079391\n","MS loss: 0.072789\n","MS loss: 0.063404\n","MS loss: 0.060628\n","MS loss: 0.050093\n","MS loss: 0.090972\n","MS loss: 0.055122\n","MS loss: 0.050806\n","MS loss: 0.084385\n","MS loss: 0.077991\n","MS loss: 0.089951\n","MS loss: 0.080343\n","MS loss: 0.074570\n","MS loss: 0.058051\n","MS loss: 0.074338\n","MS loss: 0.091156\n","MS loss: 0.106638\n","MS loss: 0.071707\n","MS loss: 0.083549\n","Elapsed 48.13929104804993\n","Train: Epoch = 35 | Loss = 1.7244306632450648 | Accuracy = 0.52\n","ms avg epoch loss: 0.0760716503219945\n","MS loss: 0.147419\n","MS loss: 0.180890\n","MS loss: 0.108463\n","MS loss: 0.096966\n","MS loss: 0.097339\n","MS loss: 0.090849\n","MS loss: 0.078200\n","MS loss: 0.072269\n","Val: Epoch = 35 | Loss 1.4369223564863205 | Accuracy = 0.543859649122807\n","Ms Epoch: 0.10904945991933346\n","MS loss: 0.082279\n","MS loss: 0.094127\n","MS loss: 0.076216\n","MS loss: 0.107010\n","MS loss: 0.073157\n","MS loss: 0.087040\n","MS loss: 0.105643\n","MS loss: 0.063050\n","MS loss: 0.063659\n","MS loss: 0.083116\n","MS loss: 0.091285\n","MS loss: 0.065919\n","MS loss: 0.083986\n","MS loss: 0.057963\n","MS loss: 0.115292\n","MS loss: 0.107325\n","MS loss: 0.083777\n","MS loss: 0.041783\n","MS loss: 0.051242\n","MS loss: 0.071262\n","MS loss: 0.018115\n","Elapsed 48.192821979522705\n","Train: Epoch = 36 | Loss = 1.46911244732993 | Accuracy = 0.5661538461538461\n","ms avg epoch loss: 0.0772974452979508\n","MS loss: 0.100358\n","MS loss: 0.059390\n","MS loss: 0.089085\n","MS loss: 0.076290\n","MS loss: 0.112884\n","MS loss: 0.102476\n","MS loss: 0.061397\n","MS loss: 0.089239\n","MS loss: 0.054381\n","MS loss: 0.069367\n","MS loss: 0.078698\n","MS loss: 0.092219\n","MS loss: 0.063439\n","MS loss: 0.083116\n","MS loss: 0.074551\n","MS loss: 0.067160\n","MS loss: 0.053656\n","MS loss: 0.100509\n","MS loss: 0.061603\n","MS loss: 0.081775\n","MS loss: 0.074971\n","Elapsed 47.343344926834106\n","Train: Epoch = 37 | Loss = 1.4022682309150696 | Accuracy = 0.5907692307692308\n","ms avg epoch loss: 0.07840780399384953\n","MS loss: 0.071357\n","MS loss: 0.044603\n","MS loss: 0.100518\n","MS loss: 0.050914\n","MS loss: 0.090164\n","MS loss: 0.101361\n","MS loss: 0.056873\n","MS loss: 0.094492\n","MS loss: 0.058385\n","MS loss: 0.072542\n","MS loss: 0.083808\n","MS loss: 0.088596\n","MS loss: 0.055123\n","MS loss: 0.083412\n","MS loss: 0.057427\n","MS loss: 0.108892\n","MS loss: 0.054490\n","MS loss: 0.120241\n","MS loss: 0.060698\n","MS loss: 0.077519\n","MS loss: 0.054321\n","Elapsed 47.958434104919434\n","Train: Epoch = 38 | Loss = 1.5605458532060896 | Accuracy = 0.556923076923077\n","ms avg epoch loss: 0.07551133703617822\n","MS loss: 0.075489\n","MS loss: 0.097281\n","MS loss: 0.096707\n","MS loss: 0.090070\n","MS loss: 0.075038\n","MS loss: 0.095191\n","MS loss: 0.097965\n","MS loss: 0.083682\n","MS loss: 0.114003\n","MS loss: 0.096697\n","MS loss: 0.090952\n","MS loss: 0.061381\n","MS loss: 0.040113\n","MS loss: 0.073921\n","MS loss: 0.121540\n","MS loss: 0.099423\n","MS loss: 0.080752\n","MS loss: 0.082067\n","MS loss: 0.055956\n","MS loss: 0.072201\n","MS loss: 0.032597\n","Elapsed 47.96447682380676\n","Train: Epoch = 39 | Loss = 1.4426075929687137 | Accuracy = 0.5846153846153846\n","ms avg epoch loss: 0.08252494870906785\n","MS loss: 0.090090\n","MS loss: 0.066225\n","MS loss: 0.119947\n","MS loss: 0.090799\n","MS loss: 0.067439\n","MS loss: 0.087274\n","MS loss: 0.071692\n","MS loss: 0.100335\n","MS loss: 0.096765\n","MS loss: 0.065117\n","MS loss: 0.056458\n","MS loss: 0.037468\n","MS loss: 0.108195\n","MS loss: 0.093320\n","MS loss: 0.103823\n","MS loss: 0.105684\n","MS loss: 0.031625\n","MS loss: 0.108850\n","MS loss: 0.083677\n","MS loss: 0.096068\n","MS loss: 0.063233\n","Elapsed 48.18898868560791\n","Train: Epoch = 40 | Loss = 1.407059995901017 | Accuracy = 0.563076923076923\n","ms avg epoch loss: 0.08305162954188529\n","MS loss: 0.147410\n","MS loss: 0.180877\n","MS loss: 0.108467\n","MS loss: 0.096958\n","MS loss: 0.097342\n","MS loss: 0.090839\n","MS loss: 0.078192\n","MS loss: 0.072279\n","Val: Epoch = 40 | Loss 1.3350661993026733 | Accuracy = 0.6052631578947368\n","Ms Epoch: 0.10904561821371317\n","MS loss: 0.079217\n","MS loss: 0.044239\n","MS loss: 0.132550\n","MS loss: 0.070147\n","MS loss: 0.099706\n","MS loss: 0.046576\n","MS loss: 0.062375\n","MS loss: 0.069473\n","MS loss: 0.073837\n","MS loss: 0.063700\n","MS loss: 0.110071\n","MS loss: 0.094773\n","MS loss: 0.118536\n","MS loss: 0.069749\n","MS loss: 0.053523\n","MS loss: 0.073975\n","MS loss: 0.095402\n","MS loss: 0.055475\n","MS loss: 0.094963\n","MS loss: 0.065697\n","MS loss: 0.075409\n","Elapsed 48.27020502090454\n","Train: Epoch = 41 | Loss = 1.4541379071417309 | Accuracy = 0.5907692307692308\n","ms avg epoch loss: 0.07854258641600609\n","MS loss: 0.059177\n","MS loss: 0.102375\n","MS loss: 0.121397\n","MS loss: 0.089953\n","MS loss: 0.064890\n","MS loss: 0.073311\n","MS loss: 0.059633\n","MS loss: 0.065595\n","MS loss: 0.055780\n","MS loss: 0.076673\n","MS loss: 0.105011\n","MS loss: 0.088247\n","MS loss: 0.119063\n","MS loss: 0.076509\n","MS loss: 0.090225\n","MS loss: 0.094762\n","MS loss: 0.109168\n","MS loss: 0.058046\n","MS loss: 0.086400\n","MS loss: 0.074356\n","MS loss: 0.082352\n","Elapsed 47.888357639312744\n","Train: Epoch = 42 | Loss = 1.4773014613560267 | Accuracy = 0.5723076923076923\n","ms avg epoch loss: 0.08347254157775924\n","MS loss: 0.109508\n","MS loss: 0.107667\n","MS loss: 0.050331\n","MS loss: 0.080267\n","MS loss: 0.044386\n","MS loss: 0.072672\n","MS loss: 0.105873\n","MS loss: 0.051885\n","MS loss: 0.055454\n","MS loss: 0.069864\n","MS loss: 0.095331\n","MS loss: 0.062736\n","MS loss: 0.087821\n","MS loss: 0.086917\n","MS loss: 0.110427\n","MS loss: 0.052002\n","MS loss: 0.102544\n","MS loss: 0.094111\n","MS loss: 0.065474\n","MS loss: 0.069883\n","MS loss: 0.101200\n","Elapsed 48.14643979072571\n","Train: Epoch = 43 | Loss = 1.4373830670402163 | Accuracy = 0.5815384615384616\n","ms avg epoch loss: 0.07982629750456129\n","MS loss: 0.108824\n","MS loss: 0.108684\n","MS loss: 0.060374\n","MS loss: 0.064984\n","MS loss: 0.062474\n","MS loss: 0.073180\n","MS loss: 0.052964\n","MS loss: 0.065187\n","MS loss: 0.063616\n","MS loss: 0.068275\n","MS loss: 0.093070\n","MS loss: 0.102032\n","MS loss: 0.094377\n","MS loss: 0.056816\n","MS loss: 0.078868\n","MS loss: 0.110808\n","MS loss: 0.054588\n","MS loss: 0.097816\n","MS loss: 0.088069\n","MS loss: 0.100889\n","MS loss: 0.108667\n","Elapsed 48.07147455215454\n","Train: Epoch = 44 | Loss = 1.3349038816633678 | Accuracy = 0.6030769230769231\n","ms avg epoch loss: 0.08164583714235396\n","MS loss: 0.085262\n","MS loss: 0.079852\n","MS loss: 0.069641\n","MS loss: 0.073001\n","MS loss: 0.092547\n","MS loss: 0.064570\n","MS loss: 0.094617\n","MS loss: 0.067202\n","MS loss: 0.073061\n","MS loss: 0.087372\n","MS loss: 0.068737\n","MS loss: 0.062521\n","MS loss: 0.065194\n","MS loss: 0.117067\n","MS loss: 0.093054\n","MS loss: 0.125770\n","MS loss: 0.127547\n","MS loss: 0.065155\n","MS loss: 0.059896\n","MS loss: 0.072058\n","MS loss: 0.051043\n","Elapsed 47.58412742614746\n","Train: Epoch = 45 | Loss = 1.7125509807041712 | Accuracy = 0.5138461538461538\n","ms avg epoch loss: 0.08072225447921526\n","MS loss: 0.147412\n","MS loss: 0.180873\n","MS loss: 0.108468\n","MS loss: 0.096952\n","MS loss: 0.097340\n","MS loss: 0.090847\n","MS loss: 0.078204\n","MS loss: 0.072320\n","Val: Epoch = 45 | Loss 1.7001722678542137 | Accuracy = 0.49122807017543857\n","Ms Epoch: 0.10905204433947802\n","MS loss: 0.066607\n","MS loss: 0.086063\n","MS loss: 0.105285\n","MS loss: 0.065374\n","MS loss: 0.090002\n","MS loss: 0.091554\n","MS loss: 0.054593\n","MS loss: 0.099700\n","MS loss: 0.069516\n","MS loss: 0.072244\n","MS loss: 0.064482\n","MS loss: 0.083155\n","MS loss: 0.076273\n","MS loss: 0.072676\n","MS loss: 0.085773\n","MS loss: 0.095949\n","MS loss: 0.112225\n","MS loss: 0.072523\n","MS loss: 0.096564\n","MS loss: 0.080469\n","MS loss: 0.073428\n","Elapsed 47.83692479133606\n","Train: Epoch = 46 | Loss = 1.4907323916753132 | Accuracy = 0.5815384615384616\n","ms avg epoch loss: 0.08164079043836821\n","MS loss: 0.086132\n","MS loss: 0.111935\n","MS loss: 0.044656\n","MS loss: 0.070707\n","MS loss: 0.068003\n","MS loss: 0.086060\n","MS loss: 0.080143\n","MS loss: 0.056852\n","MS loss: 0.096284\n","MS loss: 0.092905\n","MS loss: 0.050668\n","MS loss: 0.083890\n","MS loss: 0.080575\n","MS loss: 0.091527\n","MS loss: 0.041073\n","MS loss: 0.119719\n","MS loss: 0.079086\n","MS loss: 0.088093\n","MS loss: 0.079258\n","MS loss: 0.069979\n","MS loss: 0.054999\n","Elapsed 47.75074100494385\n","Train: Epoch = 47 | Loss = 1.3270383136613029 | Accuracy = 0.6369230769230769\n","ms avg epoch loss: 0.07774017201293082\n","MS loss: 0.100521\n","MS loss: 0.094960\n","MS loss: 0.123378\n","MS loss: 0.098334\n","MS loss: 0.064905\n","MS loss: 0.063112\n","MS loss: 0.085671\n","MS loss: 0.072060\n","MS loss: 0.057712\n","MS loss: 0.070730\n","MS loss: 0.071118\n","MS loss: 0.100170\n","MS loss: 0.108105\n","MS loss: 0.081608\n","MS loss: 0.078060\n","MS loss: 0.102776\n","MS loss: 0.087510\n","MS loss: 0.057940\n","MS loss: 0.111557\n","MS loss: 0.086607\n","MS loss: 0.069883\n","Elapsed 47.751288652420044\n","Train: Epoch = 48 | Loss = 1.2718677208537148 | Accuracy = 0.6646153846153846\n","ms avg epoch loss: 0.0850817719030948\n","MS loss: 0.063533\n","MS loss: 0.067650\n","MS loss: 0.073277\n","MS loss: 0.079105\n","MS loss: 0.077207\n","MS loss: 0.071291\n","MS loss: 0.075498\n","MS loss: 0.060982\n","MS loss: 0.061024\n","MS loss: 0.096596\n","MS loss: 0.100975\n","MS loss: 0.100273\n","MS loss: 0.069631\n","MS loss: 0.078096\n","MS loss: 0.098771\n","MS loss: 0.074174\n","MS loss: 0.059782\n","MS loss: 0.085074\n","MS loss: 0.059199\n","MS loss: 0.067818\n","MS loss: 0.105389\n","Elapsed 47.85789084434509\n","Train: Epoch = 49 | Loss = 1.390218626885187 | Accuracy = 0.6123076923076923\n","ms avg epoch loss: 0.0773974818487962\n","MS loss: 0.095763\n","MS loss: 0.080681\n","MS loss: 0.091852\n","MS loss: 0.057184\n","MS loss: 0.096872\n","MS loss: 0.095023\n","MS loss: 0.108624\n","MS loss: 0.076683\n","MS loss: 0.078322\n","MS loss: 0.091121\n","MS loss: 0.107188\n","MS loss: 0.078235\n","MS loss: 0.080447\n","MS loss: 0.083924\n","MS loss: 0.064326\n","MS loss: 0.055424\n","MS loss: 0.030866\n","MS loss: 0.089254\n","MS loss: 0.086806\n","MS loss: 0.100281\n","MS loss: 0.093005\n","Elapsed 47.873558044433594\n","Train: Epoch = 50 | Loss = 1.3517503227506364 | Accuracy = 0.6276923076923077\n","ms avg epoch loss: 0.08294673734122798\n","MS loss: 0.147414\n","MS loss: 0.180878\n","MS loss: 0.108472\n","MS loss: 0.096956\n","MS loss: 0.097346\n","MS loss: 0.090836\n","MS loss: 0.078194\n","MS loss: 0.072259\n","Val: Epoch = 50 | Loss 1.2783055901527405 | Accuracy = 0.6052631578947368\n","Ms Epoch: 0.10904431901872158\n","MS loss: 0.105988\n","MS loss: 0.089869\n","MS loss: 0.052713\n","MS loss: 0.087738\n","MS loss: 0.058776\n","MS loss: 0.081844\n","MS loss: 0.094929\n","MS loss: 0.071405\n","MS loss: 0.092345\n","MS loss: 0.081482\n","MS loss: 0.090813\n","MS loss: 0.058799\n","MS loss: 0.073094\n","MS loss: 0.098558\n","MS loss: 0.066740\n","MS loss: 0.064494\n","MS loss: 0.075139\n","MS loss: 0.076898\n","MS loss: 0.075378\n","MS loss: 0.098996\n","MS loss: 0.031352\n","Elapsed 47.81048631668091\n","Train: Epoch = 51 | Loss = 1.247780320190248 | Accuracy = 0.6369230769230769\n","ms avg epoch loss: 0.07749285637622788\n","MS loss: 0.135123\n","MS loss: 0.064461\n","MS loss: 0.070865\n","MS loss: 0.091599\n","MS loss: 0.075588\n","MS loss: 0.076526\n","MS loss: 0.057569\n","MS loss: 0.066666\n","MS loss: 0.100002\n","MS loss: 0.077810\n","MS loss: 0.099107\n","MS loss: 0.078634\n","MS loss: 0.136844\n","MS loss: 0.035908\n","MS loss: 0.082688\n","MS loss: 0.069251\n","MS loss: 0.051745\n","MS loss: 0.053905\n","MS loss: 0.074742\n","MS loss: 0.062786\n","MS loss: 0.031819\n","Elapsed 47.937849283218384\n","Train: Epoch = 52 | Loss = 1.0913500459421248 | Accuracy = 0.683076923076923\n","ms avg epoch loss: 0.07588751489917438\n","MS loss: 0.091197\n","MS loss: 0.068577\n","MS loss: 0.085586\n","MS loss: 0.064020\n","MS loss: 0.076733\n","MS loss: 0.079118\n","MS loss: 0.084144\n","MS loss: 0.076564\n","MS loss: 0.103728\n","MS loss: 0.069685\n","MS loss: 0.051755\n","MS loss: 0.056424\n","MS loss: 0.078052\n","MS loss: 0.088775\n","MS loss: 0.105039\n","MS loss: 0.107071\n","MS loss: 0.068191\n","MS loss: 0.129865\n","MS loss: 0.090924\n","MS loss: 0.078318\n","MS loss: 0.124231\n","Elapsed 47.36943817138672\n","Train: Epoch = 53 | Loss = 1.2362169850440252 | Accuracy = 0.6553846153846153\n","ms avg epoch loss: 0.08466652017973718\n","MS loss: 0.070249\n","MS loss: 0.081075\n","MS loss: 0.104137\n","MS loss: 0.072983\n","MS loss: 0.078914\n","MS loss: 0.094824\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6woBiHU8bwBE","colab_type":"code","colab":{}},"source":["#loss = nn.CrossEntropyLoss()\n","loss = F.cross_entropy\n","\n","a = torch.Tensor([0.5, 0.6, 0.7])\n","b = torch.LongTensor([-1, -1, -1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIbhfVwCdw3m","colab_type":"code","outputId":"4c98286a-3454-4e55-de66-e0da4c83e108","executionInfo":{"status":"error","timestamp":1591537299066,"user_tz":-120,"elapsed":7372304,"user":{"displayName":"Laboratorio MLDL","photoUrl":"","userId":"15250599834567100244"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["F.cross_entropy((a), (b))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-839d2358e18c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1533\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"]}]}]}