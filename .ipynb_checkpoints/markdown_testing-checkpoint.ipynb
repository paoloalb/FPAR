{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPAR-Net\n",
    "#### A model capable of performing first person action recognition\n",
    "\n",
    "With this model we explore different methods for performing a first person action recognition task on the GTEA61\n",
    "dataset. We start by implementing a structure based on Ego-RNN: a two stream architecture that works separately\n",
    "on the frames of the videos and on motion features extracted from optical flow. We then try to improve on top of this architecture by implementing a self-supervised task capable of working jointly on spatial and temporal features. Finally, we implement a two-in-one stream architecture, embedding RGB and optical flow into a single stream\n",
    "\n",
    "## Ego-RNN\n",
    "\n",
    "[Ego-RNN](https://github.com/swathikirans/ego-rnn) is a network used in the 2018 paper: \"Attention is All We Need: Nailing Down Object-centric Attention for Egocentric Activity Recognition\", and it is used as the starting point for our architecture.\n",
    "![](images/RGB_ms_task.svg)\n",
    "\n",
    "Attentioned with no MS task         |  Attentioned \n",
    ":-------------------------:|:-------------------------:\n",
    "![](gifs/close_jam_no_ms.gif)| ![](gifs/close_jam.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
